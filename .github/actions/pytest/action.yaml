# Test Runner Action
#
# This composite action executes Python tests with pytest, providing
# comprehensive test execution and reporting capabilities.
#
# Key Features:
# - Multiple test type support
# - Parallel execution
# - Coverage reporting
# - Performance tracking
# - Result analysis
#
# Process Stages:
# 1. Environment Setup:
#    - Python configuration
#    - Virtual environment creation
#    - Dependency installation
#
# 2. Test Execution:
#    - Test scope determination
#    - Parallel processing
#    - Coverage tracking
#    - Performance monitoring
#
# 3. Results Processing:
#    - Coverage analysis
#    - Performance reporting
#    - Results aggregation
#
# Required Inputs:
# - python-version: Python version for tests
# - test-type: Type of tests to run
# - codecov-token: Token for coverage upload
# - max-test-time: Maximum test duration
# - device: Device to run tests on (cpu/gpu)
#
# Outputs:
# - coverage-percentage: Total coverage
# - tests-passed: Test success status
# - test-duration: Execution time
#
# Example Usage:
# steps:
#   - uses: ./.github/actions/pytest
#     with:
#       python-version: "3.11"
#       test-type: "unit"
#       codecov-token: ${{ secrets.CODECOV_TOKEN }}
#
# Note: Requires proper pytest configuration in pyproject.toml

name: "Python Tests Runner"
description: "Runs Python unit and integration tests with pytest and uploads coverage to Codecov"

inputs:
  python-version:
    description: "Python version to use"
    required: false
    default: "3.10"
  test-type:
    description: "Type of tests to run (unit/integration/all)"
    required: false
    default: "all"
  codecov-token:
    description: "Codecov upload token"
    required: true
  max-test-time:
    description: "Maximum time in seconds for the test suite to run"
    required: false
    default: "300"
  device:
    description: "Device to run tests on (cpu/gpu)"
    required: false
    default: "gpu"

outputs:
  coverage-percentage:
    description: "Total coverage percentage"
    value: ${{ steps.coverage.outputs.percentage }}
  tests-passed:
    description: "Whether all tests passed"
    value: ${{ steps.test-execution.outputs.success }}
  test-duration:
    description: "Total test duration in seconds"
    value: ${{ steps.test-execution.outputs.duration }}

runs:
  using: composite
  steps:
    # Set up Python with pip caching
    - name: Set up Python environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ inputs.python-version }}
        cache: pip # Enable pip caching
        cache-dependency-path: pyproject.toml

    # Create and configure virtual environment
    - name: Configure virtual environment
      id: setup-venv
      shell: bash
      run: |
        # Create isolated test environment
        python -m venv .venv
        source .venv/bin/activate
        # Install dependencies with dev extras
        python -m pip install --upgrade pip
        pip install ".[dev]"
        pip install codecov

    # Determine which tests to run based on input
    - name: Determine test scope
      id: test-scope
      shell: bash
      run: |
        case "${{ inputs.test-type }}" in
          "unit")
            echo "path=tests/unit" >> $GITHUB_OUTPUT
            ;;
          "integration")
            echo "path=tests/integration" >> $GITHUB_OUTPUT
            ;;
          *)
            # Run both test types if not specified
            echo "path=tests/unit tests/integration" >> $GITHUB_OUTPUT
            ;;
        esac

    # Execute test suite with performance tracking
    - name: Execute test suite
      id: test-execution
      shell: bash
      continue-on-error: true
      run: |
        source .venv/bin/activate
        start_time=$(date +%s)

        # Set device-specific pytest arguments
        if [ "${{ inputs.device }}" = "cpu" ]; then
          DEVICE_ARGS="-m cpu"
        else
          DEVICE_ARGS="-m 'cpu or gpu'"  # Run all tests on GPU
        fi

        # Run pytest
        PYTHONPATH=src pytest ${{ steps.test-scope.outputs.path }} \
          --numprocesses=0 \
          --durations=10 \
          --durations-min=1.0 \
          --timeout=${{ inputs.max-test-time }} \
          --verbosity=0 \
          ${DEVICE_ARGS}

        test_exit_code=$?
        echo "success=$test_exit_code" >> $GITHUB_OUTPUT

        end_time=$(date +%s)
        duration=$((end_time - start_time))
        echo "duration=$duration" >> $GITHUB_OUTPUT

        # Exit with the test result - this will set the step status but won't stop the workflow
        exit $test_exit_code

    # Always fail the workflow if tests failed, but after all steps complete
    - name: Check test results
      if: always() && steps.test-execution.outcome != 'success'
      shell: bash
      run: exit 1

    # Analyze test performance
    - name: Check test duration
      if: always()
      shell: bash
      run: |
        duration="${{ steps.test-execution.outputs.duration }}"
        echo "Test Duration: ${duration:-0} seconds"

        # Warn if tests exceed time limit
        if [ -n "$duration" ] && [ "$duration" -gt "${{ inputs.max-test-time }}" ]; then
          echo "::warning::Test suite exceeded recommended duration of ${{ inputs.max-test-time }} seconds"
        fi

    # Upload coverage data to Codecov
    - name: Upload coverage to Codecov
      if: steps.test-execution.outputs.success == 'true'
      shell: bash
      run: |
        source .venv/bin/activate
        # Upload with test type and Python version tags
        codecov --token "${{ inputs.codecov-token }}" \
                --file coverage.xml \
                --flags "${{ inputs.test-type }}_py${{ inputs.python-version }}" \
                --name "${{ inputs.test-type }} tests (Python ${{ inputs.python-version }})"
